<h1>Web Speech API</h1><div><div class="notice experimental"> <p> <strong>This is an experimental technology</strong><br>Because this technology's specification has not stabilized, check the <a href="#Browser_compatibility">compatibility table</a> for usage in various browsers. Also note that the syntax and behavior of an experimental technology is subject to change in future versions of browsers as the specification changes.</p> </div></div> <div class="summary"> <p>The Web Speech API enables you to incorporate voice data into web apps. The Web Speech API has two parts: SpeechSynthesis (Text-to-Speech), and SpeechRecognition (Asynchronous Speech Recognition.)</p> </div> <h2 id="Web_Speech_Concepts_and_Usage">Web Speech Concepts and Usage</h2> <p>The Web Speech API makes web apps able to handle voice data. There are two components to this API:</p> <ul> <li>Speech recogition is accessed via the <a href="speechrecognition" title="The SpeechRecognition interface of the Web Speech API is the controller interface for the recognition service; this also handles the SpeechRecognitionEvent sent from the recognition service."><code>SpeechRecognition</code></a> interface, which provides the ability to recognize voice context from an audio input (normally via the device's default speech recognition service) and respond appropriately. Generally you'll use the interface's constructor to create a new <a href="speechrecognition" title="The SpeechRecognition interface of the Web Speech API is the controller interface for the recognition service; this also handles the SpeechRecognitionEvent sent from the recognition service."><code>SpeechRecognition</code></a> object, which has a number of event handlers available for detecting when speech is input through the device's microphone. The <a href="speechgrammar" title="The SpeechGrammar interface of the Web Speech API represents a set of words or patterns of words that we want the recognition service to recognize."><code>SpeechGrammar</code></a> interface represents a container for a particular set of grammar that your app should recognise. Grammar is defined using <a href="http://www.w3.org/TR/jsgf/">JSpeech Grammar Format</a> (<strong>JSGF</strong>.)</li> <li>Speech synthesis is accessed via the <a href="speechsynthesis" title="The SpeechSynthesis interface of the Web Speech API is the controller interface for the speech service; this can be used to retrieve information about the synthesis voices available on the device, start and pause speech, and other commands besides."><code>SpeechSynthesis</code></a> interface, a text-to-speech component that allows programs to read out their text content (normally via the device's default speech synthesiser.) Different voice types are represented by <a href="speechsynthesisvoice" title="The SpeechSynthesisVoice interface of the Web Speech API represents a voice that the system supports. Every SpeechSynthesisVoice has its own relative speech service including information about language, name and URI."><code>SpeechSynthesisVoice</code></a> objects, and different parts of text that you want to be spoken are represented by <a href="speechsynthesisutterance" title="The SpeechSynthesisUtterance interface of the Web Speech API represents a speech request. It contains the content the speech service should read and information about how to read it (e.g. language, pitch and volume.)"><code>SpeechSynthesisUtterance</code></a> objects. You can get these spoken by passing them to the <a href="speechsynthesis/speak" title="The speak() method of the SpeechSynthesis interface adds an utterance to the utterance queue; it will be spoken when any other utterances queued before it have been spoken."><code>SpeechSynthesis.speak()</code></a> method.</li> </ul> <p>For more details on using these features, see <a href="web_speech_api/using_the_web_speech_api">Using the Web Speech API</a>.</p> <h2 id="Web_Speech_API_Interfaces">Web Speech API Interfaces</h2> <h3 id="Speech_recognition">Speech recognition</h3> <dl> <dt><a href="speechrecognition" title="The SpeechRecognition interface of the Web Speech API is the controller interface for the recognition service; this also handles the SpeechRecognitionEvent sent from the recognition service."><code>SpeechRecognition</code></a></dt> <dd>The controller interface for the recognition service; this also handles the <a href="speechrecognitionevent" title="The SpeechRecognitionEvent interface of the Web Speech API represents the event object for the result and nomatch events, and contains all the data associated with an interim or final speech recognition result."><code>SpeechRecognitionEvent</code></a> sent from the recognition service.</dd> <dt><a href="speechrecognitionalternative" title="The SpeechRecognitionAlternative interface of the Web Speech API represents a single word that has been recognised by the speech recognition service."><code>SpeechRecognitionAlternative</code></a></dt> <dd>Represents a single word that has been recognised by the speech recognition service.</dd> <dt><a href="speechrecognitionerror" title="The SpeechRecognitionError interface of the Web Speech API represents error messages from the recognition service."><code>SpeechRecognitionError</code></a></dt> <dd>Represents error messages from the recognition service.</dd> <dt><a href="speechrecognitionevent" title="The SpeechRecognitionEvent interface of the Web Speech API represents the event object for the result and nomatch events, and contains all the data associated with an interim or final speech recognition result."><code>SpeechRecognitionEvent</code></a></dt> <dd>The event object for the <code><a href="https://developer.mozilla.org/en-US/docs/Web/Events/result" title="/en-US/docs/Web/Events/result">result</a></code> and <code><a href="https://developer.mozilla.org/en-US/docs/Web/Events/nomatch" title="/en-US/docs/Web/Events/nomatch">nomatch</a></code> events, and contains all the data associated with an interim or final speech recognition result.</dd> <dt><a href="speechgrammar" title="The SpeechGrammar interface of the Web Speech API represents a set of words or patterns of words that we want the recognition service to recognize."><code>SpeechGrammar</code></a></dt> <dd>The words or patterns of words that we want the recognition service to recognize.</dd> <dt><a href="speechgrammarlist" title="The SpeechGrammarList interface of the Web Speech API represents a list of SpeechGrammar objects containing words or patterns of words that we want the recognition service to recognize."><code>SpeechGrammarList</code></a></dt> <dd>Represents a list of <a href="speechgrammar" title="The SpeechGrammar interface of the Web Speech API represents a set of words or patterns of words that we want the recognition service to recognize."><code>SpeechGrammar</code></a> objects.</dd> <dt><a href="speechrecognitionresult" title="The SpeechRecognitionResult interface of the Web Speech API represents a single recognition match, which may contain multiple SpeechRecognitionAlternative objects."><code>SpeechRecognitionResult</code></a></dt> <dd>Represents a single recognition match, which may contain multiple <a href="speechrecognitionalternative" title="The SpeechRecognitionAlternative interface of the Web Speech API represents a single word that has been recognised by the speech recognition service."><code>SpeechRecognitionAlternative</code></a> objects.</dd> <dt><a href="speechrecognitionresultlist" title="The SpeechRecognitionResultList interface of the Web Speech API represents a list of SpeechRecognitionResult objects, or a single one if results are being captured in continuous mode."><code>SpeechRecognitionResultList</code></a></dt> <dd>Represents a list of <a href="speechrecognitionresult" title="The SpeechRecognitionResult interface of the Web Speech API represents a single recognition match, which may contain multiple SpeechRecognitionAlternative objects."><code>SpeechRecognitionResult</code></a> objects, or a single one if results are being captured in <a href="speechrecognition/continuous" title="The continuous property of the SpeechRecognition interface controls whether continuous results are returned for each recognition, or only a single result."><code>continuous</code></a> mode.</dd> </dl> <h3 id="Speech_synthesis">Speech synthesis</h3> <dl> <dt><a href="speechsynthesis" title="The SpeechSynthesis interface of the Web Speech API is the controller interface for the speech service; this can be used to retrieve information about the synthesis voices available on the device, start and pause speech, and other commands besides."><code>SpeechSynthesis</code></a></dt> <dd>The controller interface for the speech service; this can be used to retrieve information about the synthesis voices available on the device, start and pause speech, and other commands besides.</dd> <dt><a href="speechsynthesiserrorevent" title="The SpeechSynthesisErrorEvent interface of the Web Speech API contains information about any errors that occur while processing SpeechSynthesisUtterance objects in the speech service."><code>SpeechSynthesisErrorEvent</code></a></dt> <dd>Contains information about any errors that occur while processing <a href="speechsynthesisutterance" title="The SpeechSynthesisUtterance interface of the Web Speech API represents a speech request. It contains the content the speech service should read and information about how to read it (e.g. language, pitch and volume.)"><code>SpeechSynthesisUtterance</code></a> objects in the speech service.</dd> <dt><a href="speechsynthesisevent" title="The SpeechSynthesisEvent interface of the Web Speech API contains information about the current state of SpeechSynthesisUtterance objects that have been processed in the speech service."><code>SpeechSynthesisEvent</code></a></dt> <dd>Contains information about the current state of <a href="speechsynthesisutterance" title="The SpeechSynthesisUtterance interface of the Web Speech API represents a speech request. It contains the content the speech service should read and information about how to read it (e.g. language, pitch and volume.)"><code>SpeechSynthesisUtterance</code></a> objects that have been processed in the speech service.</dd> <dt><a href="speechsynthesisutterance" title="The SpeechSynthesisUtterance interface of the Web Speech API represents a speech request. It contains the content the speech service should read and information about how to read it (e.g. language, pitch and volume.)"><code>SpeechSynthesisUtterance</code></a></dt> <dd>Represents a speech request. It contains the content the speech service should read and information about how to read it (e.g. language, pitch and volume.)</dd> </dl> <dl> <dt><a href="speechsynthesisvoice" title="The SpeechSynthesisVoice interface of the Web Speech API represents a voice that the system supports. Every SpeechSynthesisVoice has its own relative speech service including information about language, name and URI."><code>SpeechSynthesisVoice</code></a></dt> <dd>Represents a voice that the system supports. Every <code>SpeechSynthesisVoice</code> has its own relative speech service including information about language, name and URI.</dd> <dt><a href="window/speechsynthesis" title="The speechSynthesis read-only property of the Window object returns a SpeechSynthesis object, which is the entry point into using Web Speech API speech synthesis functionality."><code>Window.speechSynthesis</code></a></dt> <dd>Specced out as part of a <code>[NoInterfaceObject]</code> interface called <code>SpeechSynthesisGetter</code>, and Implemented by the <code>Window</code> object, the <code>speechSynthesis</code> property provides access to the <a href="speechsynthesis" title="The SpeechSynthesis interface of the Web Speech API is the controller interface for the speech service; this can be used to retrieve information about the synthesis voices available on the device, start and pause speech, and other commands besides."><code>SpeechSynthesis</code></a> controller, and therefore the entry point to speech synthesis functionality.</dd> </dl> <h2 id="Examples">Examples</h2> <p>The <a href="https://github.com/mdn/web-speech-api/">Web Speech API repo</a> on GitHub contains demos to illustrate speech recognition and synthesis.</p> <h2 id="Specifications">Specifications</h2> <table class="standard-table"> <tbody> <tr> <th scope="col">Specification</th> <th scope="col">Status</th> <th scope="col">Comment</th> </tr> <tr> <td><a href="https://dvcs.w3.org/hg/speech-api/raw-file/tip/webspeechapi.html" class="external" lang="en" title="The 'Web Speech API' specification" hreflang="en">Web Speech API</a></td> <td><span class="spec-Draft">Draft</span></td> <td>Initial definition</td> </tr> </tbody> </table> <h2 id="Browser_compatibility">Browser compatibility</h2>  <div id="compat-desktop"> <table class="compat-table"> <tbody> <tr> <th>Feature</th> <th>Chrome</th> <th>Firefox (Gecko)</th> <th>Internet Explorer</th> <th>Opera</th> <th>Safari (WebKit)</th> </tr> <tr> <td>Basic support</td> <td>33<sup>[1]</sup>
</td> <td>
<a href="https://developer.mozilla.org/en-US/Firefox/Releases/49" title="Released on 2016-09-13.">49</a> (49)<sup>[2]</sup>
</td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> </tr> </tbody> </table> </div> <div id="compat-mobile"> <table class="compat-table"> <tbody> <tr> <th>Feature</th> <th>Android</th> <th>Chrome</th> <th>Firefox Mobile (Gecko)</th> <th>Firefox OS</th> <th>IE Phone</th> <th>Opera Mobile</th> <th>Safari Mobile</th> </tr> <tr> <td>Basic support</td> <td><span title="Compatibility unknown; please update this." style="color: rgb(255, 153, 0);">?</span></td> <td>
<span title="Please update this with the earliest version of support." style="color: #888;">(Yes)</span><sup>[1]</sup>
</td> <td><span title="Compatibility unknown; please update this." style="color: rgb(255, 153, 0);">?</span></td> <td>2.5</td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> <td><span style="color: #f00;">No support</span></td> </tr> </tbody> </table> </div> <ul> <li>[1] Speech recognition interfaces are currently prefixed in Chrome, so you'll need to prefix interface names appropriately, e.g. <code>webkitSpeechRecognition</code>; You'll also need to serve your code through a web server for recognition to work. Speech synthesis is fully supported without prefixes.</li> <li>[2] Recognition can be enabled via the <code>media.webspeech.recognition.enable</code> flag in <code>about:config</code>; synthesis is switched on by default. Note that currently only the speech synthesis part is available in Firefox Desktop — the speech recognition part will be available soon, once the required internal permissions are sorted out.</li> </ul> <h2 id="Firefox_OS_permissions">Firefox OS permissions</h2> <p>To use speech recognition in an app, you need to specify the following permissions in your <a href="https://developer.mozilla.org/en-US/docs/Web/Apps/Build/Manifest">manifest</a>:</p> <pre data-language="json">"permissions": {
  "audio-capture" : {
    "description" : "Audio capture"
  },
  "speech-recognition" : {
    "description" : "Speech recognition"
  }
}</pre> <p>You also need a privileged app, so you need to include this as well:</p> <pre data-language="json">  "type": "privileged"</pre> <p>Speech synthesis needs no permissions to be set.</p> <h2 id="See_also">See also</h2> <ul> <li><a href="web_speech_api/using_the_web_speech_api">Using the Web Speech API</a></li> <li><a href="http://www.sitepoint.com/talking-web-pages-and-the-speech-synthesis-api/">SitePoint article</a></li> <li><a href="http://updates.html5rocks.com/2014/01/Web-apps-that-talk---Introduction-to-the-Speech-Synthesis-API">HTML5Rocks article</a></li> <li>
<a href="http://aurelio.audero.it/demo/speech-synthesis-api-demo.html">Demo</a> [aurelio.audero.it]</li> </ul><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API$edit" class="_attribution-link">Edit this page on MDN</a>
  </p>
</div>
<div class="_attribution">
  <p class="_attribution-p">
    &copy; 2005&ndash;2017 Mozilla Developer Network and individual contributors.<br>Licensed under the Creative Commons Attribution-ShareAlike License v2.5 or later.<br>
    <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API" class="_attribution-link">https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API</a>
  </p>
</div>
